\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, amsthm, enumerate, graphicx}
\usepackage[usenames,dvipsnames]{color}
\usepackage{bm}
\usepackage[strings]{underscore}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{minted}
\usepackage{enumerate}
\usepackage{fancyvrb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{tikzsymbols}
\usepackage{todonotes}
\usepackage[most]{tcolorbox}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{float}
\usepackage{graphics}
\setlength{\marginparwidth}{2.15cm}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{epsfig}
\usepackage{setspace}
\usepackage{parskip}
\usepackage[normalem]{ulem}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows, automata}
\usepackage[font=scriptsize]{subcaption}
\usepackage{float}

\usepackage{environ}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{url}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{lastpage}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{multicol}
\usepackage{tabularx}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{datetime}

\usepackage{array}
\usepackage{multirow}
\usepackage{wasysym}
\usepackage{cancel}

\usepackage{listings}
\usepackage{color}
\usepackage[thinlines]{easytable}
\usepackage{lastpage}

\newcommand{\R}{\mathbb{R}}
\newcommand{\blackcircle}{\tikz\draw[black,fill=black] (0,0) circle (1ex);}
\renewcommand{\circle}{\tikz\draw[black] (0,0) circle (1ex);}


\usetikzlibrary{positioning,calc}


%-------------------------------------------------------------------------------
% Custom commands
\usepackage{xcolor} %hilight
\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}
%-------------------------------------------------------------------------------

%%BEGINSOLUTION
% To delete solutions from the TeX file, do:
% sed '/\%\%BEGINSOLUTION/,/\%\%ENDSOLUTION/d' main.tex > out.tex
%%ENDSOLUTION
\newtcolorbox[]{solution}[1][]{%
    breakable,
    enhanced,
    colback=white,
    title=Solution,
    #1
}


\begin{document}
\section*{}
\begin{center}
  \centerline{\textsc{\LARGE  Homework Template}}
\end{center}

Use this template to record your answers for Homework .  Add your answers using \LaTeX and then save your document as a PDF to upload to Gradescope.  You are required to use this template to submit your answers.  \textbf{You should not alter this template in any way} other than to insert your solutions.  You must submit all \pageref{LastPage} pages of this template to Gradescope.  Do not remove the instructions page(s).  Altering this template or including your solutions outside of the provided boxes can result in your assignment being graded incorrectly.

You should also export your code as a .py file and upload it to the \textbf{separate} Gradescope coding assignment. Remember to mark all teammates on \textbf{both} assignment uploads through Gradescope.

\section*{Instructions for Specific Problem Types}

On this homework, you must fill in blanks for each problem. Please make sure your final answer is fully included in the given space.  \textbf{Do not change the size of the box provided.}  For short answer questions you should \textbf{not} include your work in your solution.  Only provide an explanation or proof if specifically asked.

\begin{quote}
\textbf{Fill in the blank:} What is the course number?

\begin{tcolorbox}[fit,height=1cm, width=4cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \begin{center} \end{center}
    \end{tcolorbox}
\end{quote}

\newpage

\section*{Problem 0: Collaborators}
Enter your team members' names and Andrew IDs in the boxes below. If you worked in a team with fewer than three people, leave the extra boxes blank.

Name 1: \begin{tcolorbox}[fit,height=1cm, width=5cm, blank, borderline={1pt}{1pt},nobeforeafter]
    \begin{center}
    \vspace{3mm}
    \large{}
    \end{center}
\end{tcolorbox}
Andrew ID 1: \begin{tcolorbox}[fit,height=1cm, width=5cm, blank, borderline={1pt}{1pt},nobeforeafter]
    \begin{center}
    \vspace{3mm}
    \large{}
    \end{center}
\end{tcolorbox}
    \\
Name 2: \begin{tcolorbox}[fit,height=1cm, width=5cm, blank, borderline={1pt}{1pt},nobeforeafter]
    \begin{center}
    \vspace{3mm}
    \large{}
    \end{center}
\end{tcolorbox}
Andrew ID 2: \begin{tcolorbox}[fit,height=1cm, width=5cm, blank, borderline={1pt}{1pt},nobeforeafter]
    \begin{center}
    \vspace{3mm}
    %solution
    \end{center}
\end{tcolorbox} \\
Name 3: \begin{tcolorbox}[fit,height=1cm, width=5cm, blank, borderline={1pt}{1pt},nobeforeafter]
    \begin{center}
    \vspace{3mm}
    \large{}
    \end{center}
\end{tcolorbox}
Andrew ID 3: \begin{tcolorbox}[fit,height=1cm, width=5cm, blank, borderline={1pt}{1pt},nobeforeafter]
    \begin{center}
    \vspace{3mm}
    %solution
    \end{center}
\end{tcolorbox} \\
\vspace{0.5cm}
\vspace{0.5cm}
%\begin{solution}
%\centering
%\begin{tabular}{c|c|c}
%     & name & Andrew ID \\
%     \hline
%    You                 & \hspace{10em}  &   %\hspace{6em}@andrew.cmu.edu \\
%    Collaborator 1 (optional) & \hspace{10em} & %\hspace{6em}@andrew.cmu.edu\\
%    Collaborator 2 (optional) & \hspace{10em} & %\hspace{6em}@andrew.cmu.edu\\
%\end{tabular}
%\end{solution}

Add contents here

\newpage


\section*{Introduction}
In this assignment we will explore how we can make value-based RL algorithms (in particular TD3 from Homework 3) work in the Offline RL setting - when we have access to a static dataset of environment interactions collected with some behavior policy, and no online access to the environment for learning.  In particular, we will implement behavior regularization and Conservative Q-Learning.

Note - Make sure to take a look at the \texttt{ReadMe.md} to set up your conda environment for this homework assignment.

\section*{Problem 0: Collaborators}
Please list your name and Andrew ID, as well as the names and Andrew IDs of your collaborators.

\section*{Problem 1: TD3 + Behavior Regularization (50 pts)}

In this homework, we will study the Offline RL setting, in which we want to learn a policy given a static dataset of previous interactions with the environment $\mathcal{D} = \{(s, a, r, s')\}$ instead of learning by directly interacting with the environment. Our first step will be understanding what goes wrong if we simply keep using the same algorithms as before.

\subsection*{Problem 1.1: Loading the offline dataset into the replay buffer}

The implementations in this homework will build on top of the TD3 code from Homework 3 (you should, however, start from the new starter code for Homework 6). We would like to run the critic and policy updates from TD3 using data uniformly sampled from the offline dataset (as opposed to from the online replay buffer normally used for TD3).

The offline dataset we are going to use comes from \href{https://minari.farama.org/}{Minari}, an Offline RL tasks library. In particular, we are going to use \href{https://minari.farama.org/datasets/mujoco/hopper/medium-v0/}{\texttt{hopper-medium-v0}}, a classic Offline RL task. In \texttt{utils.py}, implement \texttt{get_buffer_and_environments_for_task}, which loads data from the offline dataset into the replay buffer, and creates the environment in which we will evaluate the policies we train.

\textit{Note:} for this assignment, we will not collect any online data into the replay buffer (i.e., the only data we will have in the replay buffer will come from the static offline dataset). Therefore, the size of the replay buffer can simply be the number of timesteps in the offline dataset.


\subsection*{Problem 1.2: Running TD3 without any offline learning modifications}

Run vanilla TD3 on the \texttt{hopper-medium-v0} task with the default hyperparameters, and plot the resulting learning curves including Evaluation Returns and predicted Q-Values for dataset state-action pairs.\\

In our reference implementation, we get evaluation returns of around 10 and Q-values of more than 3000 by step 50k. These trends of returns and Q-values give us a clue about what is going wrong here. What insights can we draw from evaluation returns being low but predicted Q-values being high? \textbf{Questions:} 

\paragraph{Command (30 min):}
\begin{verbatim}
python runner.py --agent td3 --total_steps 100000
\end{verbatim}

\begin{solution}[height=5cm]
% TODO: Put your figures
% YOUR SOLUTION
\end{solution}


\subsection*{Problem 1.3: Behavior Regularized TD3}

Next, add a behavior-cloning loss to the TD3 policy objective. The policy objective from vanilla TD3 (which comes from DDPG) is given by:

\begin{equation}
\textstyle \mathcal{L}_\pi^{(\text{DDPG})} = -\mathbb{E}_{s\sim\mathcal{D}}\,[Q_1(s, \mu_\theta(s))].
\end{equation}

We are going to modify it by adding a Behavior-Cloning term to encourage the policy to stay close to the actions seen in the offline dataset:

\begin{equation}
\textstyle \mathcal{L}_\pi^{(\text{DDPG + BC-Reg})} = -\mathbb{E}_{s, a \sim \mathcal{D}}\,[Q_1(s, \mu_\theta(s))] \textcolor{red}{ + \alpha_\text{reg} \|a - \mu_\theta(s)\|_2^2}.
\end{equation}


Implement BC Regularization in \texttt{_td3_update_step} inside \texttt{td3_agent.py}, and train an agent with $\alpha_\text{reg} = 10.000$.

Our reference implementation gets evaluation returns of around 500 at 50k steps.

\begin{solution}[height=5cm]
% TODO: Put your figures
% YOUR SOLUTION
\end{solution}

\subsection*{Problem 1.4: Conceptual question about picking $\alpha_\text{reg}$}
A key choice when implementing DDPG + Behavior Regularization is the choice of the BC-regularization coefficient $\alpha_\text{reg}$. Answer the following questions:

\begin{enumerate}
    \item The effect of the value of $\alpha_\text{reg}$ is environment and dataset dependent. For which kinds of \textbf{datasets} will we expect \textbf{larger} values of $\alpha_\text{reg}$ to work \textbf{better}? And conversely, for which datasets will we expect \textbf{lower} values of $\alpha_\text{reg}$ to work better?
    \item We would like a systematic way of choosing $\alpha_\text{reg}$. Let's say that you first complete a training run with $\alpha_\text{reg} = 1.0$, and you have access to any metric you require from this training run; how would you choose a reasonable value of $\alpha_\text{reg}$ if you could only run a single more run?
\end{enumerate}

\begin{solution}[height=5cm]
% TODO: Put your figures
% YOUR SOLUTION
\end{solution}




\newpage


\section*{Problem 2: Conservative Q-Learning (50 pts)}

The behavior-cloning regularization ued in Problem 1 constraints the policy to stay close to the actions seen in the dataset, so that the policy can't exploit value overestimations. However, the Q-function itself is left unconstrained, so we might still suffer from this problem to some extent. \href{https://arxiv.org/abs/2006.04779}{Conervative Q-Learning (CQL)} addresses the issue of out-of-distribution over-estimation by \textbf{constraining the Q-function} instead of the policy. Please read the CQL paper before completing this problem.

\subsection*{Problem 2.1: Implementing the CQL loss}

In \texttt{td3_agent.py} \texttt{_get_cql_loss}, follow the instructions to implement the CQL regularization loss. In particular, implement the following regularizer that will get added to the standard TD error objective:

\begin{equation}
    \alpha \mathbb{E}_{s, a \sim \mathcal{D}} \left[\log (\sum_{a' \sim \text{uniform}} (\exp(Q(s, a'))) - Q(s, a) \right]
\end{equation}

Where $a'$ are actions sampled uniformly inside the action bounds of the environment.
Then, run training with CQL alpha 5.

\textbf{Note:} Our reference implementation got around 1.500 returns. However, depending on the implementation some seeds might lead to poor performance. If you don't get at least 1.000, re-run with 2 more seeds.

\begin{solution}[height=5cm]
% TODO: Put your figures
% YOUR SOLUTION
\end{solution}


\newpage
\section*{Problem 3: Conceptual Questions on Offline  (50 pts)}

\textbf{(i) Select all that apply and justify} regarding the offline RL methods that apply regularization on the value function for training. For this question, consider three algorithms:
    \begin{itemize}
        \item Conservative Q-Learning: as we discussed in class, this algorithm applies a regularizer to minimize large Q-values during training.
        \item Reward penalty: similar to offline model-based RL methods, we penalize the reward value at \emph{each} state-action pair by subtracting an uncertainty estimate from it.
        \item Policy constraint penalty: this approach subtracts $D_\mathrm{KL}(\pi_\theta, \pi_\beta)$ from the policy update and the value function update. 
    \end{itemize}

    Select all that apply from below:
    \begin{enumerate}
        \item Conservative Q-learning is likely to perform better than policy constraints on tasks with offline datasets that exhibit higher coverage. 
        \item Reward penalty should perform better than conservative Q-learning methods because penalizing the Q-function at each unseen action will lead to more conservatism than using uncertainty estimates to penalize the reward function.
        \item Conservative Q-learning is addressing problems of over-conservative behavior with policy constraint penalty methods.
    \end{enumerate}

\begin{tcolorbox}[fit,height=10em, width=40em, blank, borderline={1pt}{1pt},nobeforeafter]
            \begin{center}
            %YOUR SOLUTION HERE%
            \end{center}
            \end{tcolorbox}

\textbf{(ii)} In class, we studied several methods for offline RL based on policy constraints. Recall that there were two types of methods: one based on distributional constraints (e.g., optimize the Q-function subject to the KL-divergence between the learned policy and the behavior policy) and one based on support constraints (e.g., optimize the Q-function subject to a support constraint between the learned policy and the behavior policy). Describe a scenario where distribution constraint methods would be preferable to support constraints. \textit{Hint:} in class, we discussed when support constraint methods will be better. Think about when that is not the case and answer this question.

\begin{tcolorbox}[fit,height=10em, width=40em, blank, borderline={1pt}{1pt},nobeforeafter]
            \begin{center}
            %YOUR SOLUTION HERE%
            \end{center}
            \end{tcolorbox}

\textbf{(iii) Select all that apply and justify} regarding the performance of model-based offline RL methods that we studied in class. 
    \begin{enumerate}
        \item Model-based offline RL methods are generally better than model-free offline RL methods (that do not use a dynamic model), given any offline dataset.
        % \choice Even without using any form of policy constraints or reward / Q-value regularization, model-based offline RL methods can work well when the dataset exhibits high coverage.
        \item Using distributional policy constraints within a model-based offline RL methods can be helpful when the distribution of the offline dataset is \textbf{quite narrow}.
        \item Using distributional policy constraints within a model-based offline RL methods can be helpful when the distribution of the offline dataset is \textbf{quite broad and exhibits high coverage}.
    \end{enumerate}

\begin{tcolorbox}[fit,height=10em, width=40em, blank, borderline={1pt}{1pt},nobeforeafter]
            \begin{center}
            %YOUR SOLUTION HERE%
            \end{center}
            \end{tcolorbox}

\textbf{(iv)} Given a fixed static offline dataset of state-action-next state-reward transitions, why can offline RL (not filtered imitation learning) perform better than imitation learning? Give a qualitative example of a dataset where offline RL is expected to outperform imitation and one where imitation is expected to outperform offline RL.

\begin{tcolorbox}[fit,height=10em, width=40em, blank, borderline={1pt}{1pt},nobeforeafter]
            \begin{center}
            %YOUR SOLUTION HERE%
            \end{center}
            \end{tcolorbox}


\section*{Problem 4: Feedback}

\textbf{Feedback}: You can help the course staff improve the course by providing feedback. What was the most confusing part of this homework, and what would have made it less confusing?

\begin{tcolorbox}[fit,height=10em, width=40em, blank, borderline={1pt}{1pt},nobeforeafter]
            \begin{center}
            %YOUR SOLUTION HERE%
            \end{center}
            \end{tcolorbox}\\

\noindent\textbf{Time Spent}: How many hours did you spend working on this assignment? Your answer will not affect your grade.

\begin{tcolorbox}[fit,height=10em, width=40em, blank, borderline={1pt}{1pt},nobeforeafter]
\begin{table}[H]
    \centering
    \begin{tabular}{r|c}
        Alone &  \hspace{3em} %ANSWER HERE%
        \\ \hline
        With teammates & \hspace{3em} %ANSWER HERE%
        \\ \hline
        With other classmates & \hspace{3em} %ANSWER HERE%
        \\ \hline
        At office hours & \hspace{3em} %ANSWER HERE%
        \\ \hline
    \end{tabular}
\end{table}
\end{tcolorbox}


\end{document}